{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbyk8LK7aTm8+BZ6trrdYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhantgupta21/food-identifier/blob/main/food_vis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-EBATJbAIti"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=101,seed:int=42):\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # freeze all the layers and convert it into a feautre extractor model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDNP3KbnCNg-",
        "outputId": "5226346f-d161-42d9-8004-1d46959b539f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35.2M/35.2M [00:00<00:00, 81.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "foodvision_big_demo_path = Path(\"demos/foodvision/\")\n",
        "\n",
        "foodvision_big_demo_path.mkdir(parents=True,\n",
        "                               exist_ok=True)\n",
        "\n",
        "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "Vxh8ziqRDzCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile demos/foodvision/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
        "                          seed:int=42):\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. Change classifier head with random seed for reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbP1gKejFJP0",
        "outputId": "7c4fecc4-aa36-4323-dc64-9f25da563cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile demos/foodvision/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "with open(\"class_names.txt\", \"r\") as f:\n",
        "  class_names = [food_name.strip() for food_name in f.readlines()]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "# Create model and transforms\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(f=\"pretrained_effnet_foodvis.pth\",\n",
        "               map_location=torch.device(\"cpu\")) # load to CPU\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # Return pred dict and pred time\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article\n",
        "title = \"FoodVisionüçî\"\n",
        "description = \"An EfficientNetB2 feature extractor\"\n",
        "article = \"Created\"\n",
        "\n",
        "# Create example list\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # maps inputs to outputs\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGY_B01pGQHp",
        "outputId": "1d60d6ef-6ad4-492b-be27-220bd017907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting demos/foodvision/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd demos/foodvision && zip -r ../foodvision.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7xfnQ-SJBue",
        "outputId": "a4744c6d-c663-44a3-c72b-d15701b2f879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: app.py (deflated 55%)\n",
            "  adding: class_names.txt (deflated 48%)\n",
            "  adding: examples/ (stored 0%)\n",
            "  adding: examples/applepie.jpg (deflated 0%)\n",
            "  adding: examples/Pizza.jpeg (deflated 0%)\n",
            "  adding: model.py (deflated 46%)\n",
            "  adding: pretrained_effnet_foodvis.pth (deflated 8%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision.zip\")\n",
        "except:\n",
        "  print(f\"Not running in Google Colab, can't use google.colab.files.download(), please download foodvision_big.zip manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xnbBLCLpJTG7",
        "outputId": "b9082ab8-3e47-4fb0-8b16-69186f94ae28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_52f37ffc-9abe-426a-bd17-81370af30f5b\", \"foodvision.zip\", 30205622)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}